# Deeplearning_Coursea
2021 하계 딥러닝


=================
코세라 실습

1. 초반부

데이터셋 로드 및 기초변수 취하기

- 초반부의 주된 요지는 결국 데이터의 사이즈를 알맞게 조절하는 것에 있다.
- 앞서 수업에서는 RGB Layer 3겹으로 구성된 64 X 64 이미를 1열벡터 형태로 조정하고자 했었다. 이것이 1차적인 목표이다.

- Hint : 로드된 데이터 train_set_x_orig 의 Shape 는 209,64,64,3 이다.
ㄴ> 209 는 dataset 의 총 개수, 이어지는 것은 이미지 한장의 크기를 보여주고 있다.
ㄴ> 이것을 209 개의 열벡터들로 구성을 해야 하겠지? ===> Reshape 에 -1 을 매개변수로 주면 자동조정 되는 것을 잘 활용해야 함

shape[0] : 데이터셋의 개수를 암시한다
shape[1] : 각 데이터셋의 크기를 여기서 유추할 수 있겠다.

이 둘을 다른 모든 작업을 시작하기 이전에 취하고 들어간다.

데이터셋의 크기 갈무리하기

train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T

ㄴ> 잘 보자. Shape[0] 을 취해서 총 데이터셋의 개수를 얻고 있다.
ㄴ> 마지막에 .T 가 붙어있는 것으로 보아 우리는 209 개의 열벡터를 얻을 것이다.
ㄴ> 요점은 가장 앞선 차원 단위가, Reshape 의 전과 후 모두 209 라는 것이다.
ㄴ> 뒤따라오는 64 64 3 단위는 매개변수 -1 에 의해 알아서 조정된다.

sigmoid 함수

- Linear Regression : w.T * X + b 를 계산한 뒤, 이것의 범위를 0 ~ 1 로 가능한 Linear 하게 조정할 필요가 있다.
- 한 Dataset 이 하나의 Scalar 로 축소되고, 이것을 bool type 인 y 에 알맞도록 0~1사이의 yhat 으로 표준화 해주는 것이다.


표준화

- 갈무리한 데이터를 작업의 특성에 맞게 표준화 하고자 한다.
- 이미지의 경우 현재 배열의 element 에 해당하는 RGB 픽셀 이미지는 0 ~ 255 범위를 가진다.
- Regression 에 용이하도록 만들기 위해, 이것을 255 로 나누도록 한다. 이게 표준화이다.

2. 중반부

- 이제 할 작업은, 현재 세팅된 w 의 값을 가지고, 데이터셋의 input 의 추정 output 과 실제 output 을 비교하여 Cost 를 계산해내는 것이다

w & b initialize

- w 는 dim,1 모양의 매트릭스로 초기화될 필요가 있다.
ㄴ> np.zeros((dim,1))
ㄴ> zeros 안에 Shape 형태가 들어가야 하는 것이 요점이다. C++ 계통의 함수들의 구성 문화와는 다소 다른 느낌

- b 는 추후 브로드캐스팅될 스칼라이다.
ㄴ> 그냥 스칼라 하나 넣으면 되시겠다


Propagation

- 앞서 Computation Graph 에서, w & b 로부터 J 까지 이르는 연산을 최초로 정의하는 것을 Forward Propagation 이라고 하였었다.
- 그리고 J 로부터 dw & db 에 이르기까지 연산을 거꾸로 따라가는 과정을 Backword Propagation 이라고 하였다.

- 


- 자 이제 좀 머리아픈 벡터 연산을 통해 구한 것들을 정리해야 한다.
- 절차는 이러하다 - Input 을 통해 Regression 의 결과를 Dot product 로 계산한다. 
- 계산결과 1XN * NXM = 1XM 행벡터가 남는다.
- Sigmoid 를 통해 결과 행벡터를 적절히 조정한다. (* 따로 크기에 댸한 규정 없이 numpy 의 디폴트한 Broadcasting 이 작용한다)

- a(i) 가 해당 행벡터의 원소이다. y_org 는 문제에서 행벡터의 형태로 주어지므로 +- 연산 할 수 있다. Cost 를 계산한다.

- A = sigmoid(np.dot(w.T,X)+b) # Sigmoid 행벡터를 dot product 를 통해 생성한다. 
- cost = np.dot(Y * np.log(A)) + np.dot((1-Y),np.log(1-A)) # i 번째 element 끼리 곱연산 Sum 할 때에는 dot product 만한 게 없다.

- 이 뒤엔 dw 와 db 를 계산한다. 
ㄴ> v.sum(axis = 0) : x 축을 기준으로 Sum 한다. Row 들이 전부 Sum 되어 열벡터만 남는다. (Depth 가 있는 경우 , Col -> Row, Depth -> Col)
ㄴ> v.sum(axis = 1) : y 축을 기준으로 Sum 한다. Col 들이 전부 Sum 되어 행벡터만 남는다.   
ㄴ> v.sum(axis = 1) : Depth 를 기준으로 Sum 한다. 


===============

