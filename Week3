# Deeplearning_Coursea
2021 하계 딥러닝

[1], [2] 와 같은 표기로, 신경망의 Layer 들을 구분한다.

한 신경망 프로세스에서, 같은 연산( 가령, Linear Regression 과 같은) 을 여러 번 할 일 이 생길 수 있다. 
ㄴ> Sigmoid 이후 도출된 a  를 다시한번 Linear Regression 벡터연산에 집어넣는다고 한다던지

이럴 때 상기한 중괄호의 Layer 표현이 필요하다. 

Input Layer : 이전 과제에서 수직적으로 세워졌던, 1 열을 구성하는 데이터셋

Hidden Layer : 프로세싱 과정에서 나온 마찬가지의 수직적인 데이터셋 - Linear 연산 이후의 z, Sigmoid 연산 이후의 a 와 같은.
ㄴ> 실제로 Training Set 에서는 연산만 될 뿐 관측되지는 않는 값들이라고 하겠다.

Output Layer : Logistic Regression 은 이러한 결과물로 얻었던 것이 Loss Function 이었다. 이것이 전체 Datset 에 대하여 감쇠평균 된 것이 Cost 였고

각 Layer 들을 a[0] 과 같이  Activation 의 a 를 사용하여 a[n] 의 형태로 나타낸다.

일단 표기법 정리
1. 중괄호 위첨자는 몇번째 Layer 인지를 나타낸다 (데이터 Source 가 0번째부터 시작)
2. 아래첨자 i 는 해당 Layer 의 몇번째 Node 인지를 나타낸다 : 가령 Z 벡터의 i 번째 원소 계산


이걸 위첨자와 아래첨자를 사용해서 나타내야 하는데, 이 조악한 메모장 기능으로는 나타낼 수가 없다. 방법이 없을까?







신경망 - 좀더 복잡하게


- 기존의 신경망은 벡터 X와 벡터 W 를 통한 계산으로 출발했었다.
- 이번엔 W 가 벡터가 아니라 매트릭스이다. 중간 단계 계산 결과인 스칼라 z 가 이번엔 벡터이다.
- a[1] 이 전엔 원소 1개짜리 벡터였다면 이번엔 하나의 열벡터로 나온다고 보면 된다.

- 따라서 1번 레이어의 중간변수 z 역시 벡터일 것이다. a[1] 은 z 계산 이후 sigmoid 되었을 뿐이다.
- 이때 zi 들의 계산을 어쩌면 좋을까

- 각 i 번째 중간변수 zi 의 연산마다 각각 다른 W 벡터를 필요로 한다. 
- a[0] 가 원소 3개짜리 input 이라면, 기존의 W 들은 3X1 열벡터였을 것이다. 
- 우린 이것을 Transpose 하여 연산에 활용했었다.

- 이번에는, 만약 중간변수가 4개라고 하자. 

1. 각 1번 레이어의 i 번째 중간변수마다 해당하는 W 3X1 열벡터가 각각 존재한다.
2. 이것을 매트릭스 연산으로 한번에 처리하고 싶다. 그래서 Transpose 한 WT 1 X 3 행벡터들을 한 행씩 쌓는다.
3. 그럼 중간변수 총 4개에 대하여, 3 X 1 인풋에 대한 중간변수 계산을 위한 W 매트릭스는 최종적으로 4 X 3 이 될 것이다. 
4. W * a[0] 의 결과로 우린 그렇게 4 X 3 * 3 X 1 = 4 X 1 의 중간변수 벡터 Z 를 얻는다.

5. 이것이 Sigmoid 되어 a[1] 레이어를 완성시킨다.




신경망 - 거기에 M Training Examples 까지

- 이제 위 새로운, 좀더 차원이 높은 신경망에다가 M 개의 Training Example 에 대한 output 을 얻어내는 것 까지 해 보자.
- 한 Training set 를 구성하는 input 은 1개의 열벡터 였다.
- 이번엔 Input 이 이러한 열벡터들이 m 개 수평적으로 Stack 되어 있는 (= 열벡터 m 개가 양봉틀처럼 왼쪽에서 오른쪽으로 차곡차곡 쌓여있는)...
...matrix 가 인풋으로 주어진다
- 직관적으로는, 이때 모든 중간 단계의 열벡터 결과물들 (1번 레이어의 열벡터 z,  그것의 시그모이드 열벡터 a[1] 등 )이 마찬가지로..
..양봉틀 M 개 처럼 쌓이게 된다.
- 이때 W 는 열벡터와의 곱셈 1 회가 m 회가 되는 것 뿐이므로 형태에 변화가 필요가 없다.


(막간) for 문으로 하는것은 너무나도 비효율적이라서, 이러한 matrix 를 활용한 다차원 대규모 데이터 연산 능력은 반 드 시 필요하다. 





Activation Functions

- 각 Layer 들의 결과물 a[i], 각종 중간변수 z 들을 a 로 만들어 주는 (= 크기 조절을 해 주는) 함수를 Activation Function 이라고 한다.
- 사실 Activation Function 은 Sigmoid 만 쓰이는 것은 아니다.

- 우선, Binary Classification 의 마지막 Layer 에는 Sigmoid 함수가 쓰이는 것이 권고된다. 왜냐면 마지막 Layer 에서는 값의 범위를 확실히 축소시켜야 하기 떄문이다
- 하지만 중간 Layer 들 에서는, 프로그램의 비용적인 측면에서 여러가지 함수들이 쓰일 수 있다.

1. tanh : 하이퍼볼릭 탄젠트 함수이다. Sigmoid 와 유사한 개형을 지니나 중앙값이 0 이다. 많은 경우에서 중앙값이 0인 것은 여러 이점을 가져온다. 
2. Relu - Rectified Linear Unit : (0,0) 에서 꺾인 경사로 함수이다. 간단하여 가장 디폴트하게 쓰인다고 한다.
3. Leaking - Relu : 경사로가 꺾인 지점 이전, 즉 x 축의 음의 영역에서 값이 완전 0 인것이 아니라 기울기가 살짝 있는 함수이다.


Linear 한 Activation 함수에 대하여 

- 선형적인 system 들의 중첩은 결국 하나의 또다른, 더이상 복잡하지 않은 선형적인 system 이 된다. 역동적 계산이 전혀 되지 않는다는 것이다.
- 그래서 좀 덜 쓰인다. 특히 Hidden Layer 에서는 더더욱






여기까지 좀더 간단한 추상화 설명


1. 한 겹의 hidden layer 을 지니는 시스템 이다. m 회차 중 한개의 input 은 1개의 벡터로 주어진다.

2. hidden layer 의 한 중간변수 스칼라 z[i] 를 만들어 내기 위해 한 열의 w 벡터가 필요하다.
3. 그런데 중간변수 z 는 실제로는 하나의 벡터이다. 즉 스칼라 N 개를 만들어 내기 위해 N 열의 w 벡터들 => 즉 W matrix 가 필요하게 되는 것이다
4. 그래서 우리는 중간변수 벡터를 만들어 내기 위해 항상 중간 matrix 가 필요함을 인지하도록 한다.
ㄴ> W matrix 는, 이러한 개별 w 벡터들을..
- transpose 하여 행벡터로 만들고
- 새로로 stack 한 것
...임을 기억하도록 한다

5. Propagation : input -> output 으로 가는 수식의 흐름을 파악하는 것을 말한다.
6. Back - Propagataion : 각 Layer의 W matrix (+b vector) 들에 대한 J (=cost, 또는 결과물) 의 미분을 구하기 위해, 
output->input 으로 수식의 흐름을 따라가는 것을 말한다 


Random Initialization

- 가령 우리가, 어떤 Layer 에서 tanh 또는 sigomid 를 Activation Function 으로 사용하고 있다고 전제해 보자.
- 너무 큰 W 에 의해서, z 의 값이 만약 양쪽 극단(ex: 너무 작거나 혹은 너무 크거나) 에 가까이 위치해 있다고 해 보자.

- 알다시피 tanh 그리고 sigmoid 의 경우 양쪽 극단은 z[i] 에 대한 a[i] 의 미분이 아주 flat 하다. 
- 따라서, cost 를 구해서 W 를 수정하고자 할 때, W 의 초기값이 너무 크거나 작으면 학습의 속도가 아주 느려지게 된다.

- 또한 W 가 영행렬로 초기화 되어서도 아니되는데, 모종의 이유로 각 학습이 제대로 이루어 지지 않는다.......

- 그래서 NUMPY 의 rand 함수를 사용하여 난수를 사용하는 편이다



