# Deeplearning_Coursea
2021 하계 딥러닝

[1], [2] 와 같은 표기로, 신경망의 Layer 들을 구분한다.

한 신경망 프로세스에서, 같은 연산( 가령, Linear Regression 과 같은) 을 여러 번 할 일 이 생길 수 있다. 
ㄴ> Sigmoid 이후 도출된 a  를 다시한번 Linear Regression 벡터연산에 집어넣는다고 한다던지

이럴 때 상기한 중괄호의 Layer 표현이 필요하다. 

Input Layer : 이전 과제에서 수직적으로 세워졌던, 1 열을 구성하는 데이터셋

Hidden Layer : 프로세싱 과정에서 나온 마찬가지의 수직적인 데이터셋 - Linear 연산 이후의 z, Sigmoid 연산 이후의 a 와 같은.
ㄴ> 실제로 Training Set 에서는 연산만 될 뿐 관측되지는 않는 값들이라고 하겠다.

Output Layer : Logistic Regression 은 이러한 결과물로 얻었던 것이 Loss Function 이었다. 이것이 전체 Datset 에 대하여 감쇠평균 된 것이 Cost 였고

각 Layer 들을 a[0] 과 같이  Activation 의 a 를 사용하여 a[n] 의 형태로 나타낸다.

일단 표기법 정리
1. 중괄호 위첨자는 몇번째 Layer 인지를 나타낸다 (데이터 Source 가 0번째부터 시작)
2. 아래첨자 i 는 해당 Layer 의 몇번째 Node 인지를 나타낸다 : 가령 Z 벡터의 i 번째 원소 계산


이걸 위첨자와 아래첨자를 사용해서 나타내야 하는데, 이 조악한 메모장 기능으로는 나타낼 수가 없다. 방법이 없을까?







신경망 - 좀더 복잡하게


- 기존의 신경망은 벡터 X와 벡터 W 를 통한 계산으로 출발했었다.
- 이번엔 W 가 벡터가 아니라 매트릭스이다. 중간 단계 계산 결과인 스칼라 z 가 이번엔 벡터이다.
- a[1] 이 전엔 원소 1개짜리 벡터였다면 이번엔 하나의 열벡터로 나온다고 보면 된다.

- 따라서 1번 레이어의 중간변수 z 역시 벡터일 것이다. a[1] 은 z 계산 이후 sigmoid 되었을 뿐이다.
- 이때 zi 들의 계산을 어쩌면 좋을까

- 각 i 번째 중간변수 zi 의 연산마다 각각 다른 W 벡터를 필요로 한다. 
- a[0] 가 원소 3개짜리 input 이라면, 기존의 W 들은 3X1 열벡터였을 것이다. 
- 우린 이것을 Transpose 하여 연산에 활용했었다.

- 이번에는, 만약 중간변수가 4개라고 하자. 

1. 각 1번 레이어의 i 번째 중간변수마다 해당하는 W 3X1 열벡터가 각각 존재한다.
2. 이것을 매트릭스 연산으로 한번에 처리하고 싶다. 그래서 Transpose 한 WT 1 X 3 행벡터들을 한 행씩 쌓는다.
3. 그럼 중간변수 총 4개에 대하여, 3 X 1 인풋에 대한 중간변수 계산을 위한 W 매트릭스는 최종적으로 4 X 3 이 될 것이다. 
4. W * a[0] 의 결과로 우린 그렇게 4 X 3 * 3 X 1 = 4 X 1 의 중간변수 벡터 Z 를 얻는다.

5. 이것이 Sigmoid 되어 a[1] 레이어를 완성시킨다.




신경망 - 거기에 M Training Examples 까지

- 이제 위 새로운, 좀더 차원이 높은 신경망에다가 M 개의 Training Example 에 대한 output 을 얻어내는 것 까지 해 보자.
- 한 Training set 를 구성하는 input 은 1개의 열벡터 였다.
- 이번엔 Input 이 이러한 열벡터들이 m 개 수평적으로 Stack 되어 있는 (= 열벡터 m 개가 양봉틀처럼 왼쪽에서 오른쪽으로 차곡차곡 쌓여있는)...
...matrix 가 인풋으로 주어진다
- 직관적으로는, 이때 모든 중간 단계의 열벡터 결과물들 (1번 레이어의 열벡터 z,  그것의 시그모이드 열벡터 a[1] 등 )이 마찬가지로..
..양봉틀 M 개 처럼 쌓이게 된다.
- 이때 W 는 열벡터와의 곱셈 1 회가 m 회가 되는 것 뿐이므로 형태에 변화가 필요가 없다.


(막간) for 문으로 하는것은 너무나도 비효율적이라서, 이러한 matrix 를 활용한 다차원 대규모 데이터 연산 능력은 반 드 시 필요하다. 





Activation Functions

- 각 Layer 들의 결과물 a[i], 각종 중간변수 z 들을 a 로 만들어 주는 (= 크기 조절을 해 주는) 함수를 Activation Function 이라고 한다.
- 사실 Activation Function 은 Sigmoid 만 쓰이는 것은 아니다.

- 우선, Binary Classification 의 마지막 Layer 에는 Sigmoid 함수가 쓰이는 것이 권고된다. 왜냐면 마지막 Layer 에서는 값의 범위를 확실히 축소시켜야 하기 떄문이다
- 하지만 중간 Layer 들 에서는, 프로그램의 비용적인 측면에서 여러가지 함수들이 쓰일 수 있다.

1. tanh : 하이퍼볼릭 탄젠트 함수이다. Sigmoid 와 유사한 개형을 지니나 중앙값이 0 이다. 많은 경우에서 중앙값이 0인 것은 여러 이점을 가져온다. 
2. Relu - Rectified Linear Unit : (0,0) 에서 꺾인 경사로 함수이다. 간단하여 가장 디폴트하게 쓰인다고 한다.
3. Leaking - Relu : 경사로가 꺾인 지점 이전, 즉 x 축의 음의 영역에서 값이 완전 0 인것이 아니라 기울기가 살짝 있는 함수이다.


Linear 한 Activation 함수에 대하여 

- 선형적인 system 들의 중첩은 결국 하나의 또다른, 더이상 복잡하지 않은 선형적인 system 이 된다. 역동적 계산이 전혀 되지 않는다는 것이다.
- 그래서 좀 덜 쓰인다. 특히 Hidden Layer 에서는 더더욱










